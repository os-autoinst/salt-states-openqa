---
apiVersion: 1
groups:
- folder: openQA
  interval: 1m
  name: '{{ worker }}: Disk I/O time alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '577'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '56720'
      message: ''
    condition: A
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
            - 20000
            type: gt
          operator:
            type: and
          query:
            params:
            - C
          reducer:
            type: avg
        refId: A
        type: classic_conditions
      refId: A
    - datasourceUid: '000000001'
      model:
        alias: $tag_name
        groupBy:
        - params:
          - $__interval
          type: time
        - params:
          - name
          type: tag
        - params:
          - 'null'
          type: fill
        interval: 1m
        measurement: diskio
        orderByTime: ASC
        policy: default
        query: SELECT non_negative_derivative(mean("io_time"), 1s) FROM "diskio" WHERE
          ("host" = '{{ worker }}' AND ("name" <> 'nvme0n1' OR "name" = 'nvme1n1' OR "name"
          = 'sda' OR "name" = 'sdb')) AND $timeFilter GROUP BY time($__interval),
          "name" fill(null)
        rawQuery: true
        refId: C
        resultFormat: time_series
        select:
        - - params:
            - read_time
            type: field
          - params: []
            type: mean
          - params:
            - 1s
            type: non_negative_derivative
        tags:
        - key: host
          operator: =
          value: {{ worker }}
        - condition: AND
          key: name
          operator: <>
          value: nvme0n1
        - condition: OR
          key: name
          operator: =
          value: nvme1n1
        - condition: OR
          key: name
          operator: =
          value: sda
        - condition: OR
          key: name
          operator: =
          value: sdb
      refId: C
      relativeTimeRange:
        from: 300
        to: 0
    execErrState: OK
    for: 5m
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: disk_io_time_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 56720
    title: '{{ worker }}: Disk I/O time alert'
    uid: {{ (('disk_io_time_alert_' + worker ) | sha512)[:40] }}
- folder: openQA
  interval: 1m
  name: '{{ worker }}: Download rate alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '578'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '65109'
      message: |-
        Cache service download rate is lower than expected.

        See https://progress.opensuse.org/issues/106904 for details.
    condition: condition
    dashboardUid: WD{{ worker }}
    data:
    - refId: download_rate
      relativeTimeRange:
        from: 3600
        to: 0
      datasourceUid: "000000001"
      model:
        groupBy:
            - params:
                - $__interval
              type: time
            - params:
                - previous
              type: fill
        intervalMs: 1000
        maxDataPoints: 43200
        measurement: openqa_download_rate
        orderByTime: ASC
        policy: default
        query: SELECT mean("bytes") FROM "openqa_download_rate" WHERE ("host" = '{{ worker }}') AND $timeFilter GROUP BY time($__interval) fill(previous)
        rawQuery: false
        refId: download_rate
        resultFormat: time_series
        select:
            - - params:
                    - bytes
                type: field
              - params: []
                type: mean
        tags:
            - key: host
              operator: =
              value: {{ worker }}
    - refId: download_count
      relativeTimeRange:
        from: 3600
        to: 0
      datasourceUid: "000000001"
      model:
        groupBy:
            - params:
                - $__interval
              type: time
            - params:
                - previous
              type: fill
        hide: false
        intervalMs: 1000
        maxDataPoints: 43200
        measurement: openqa_download_count
        orderByTime: ASC
        policy: default
        query: SELECT mean("bytes") FROM "openqa_download_rate" WHERE ("host" = '{{ worker }}') AND $timeFilter GROUP BY time($__interval) fill(previous)
        rawQuery: false
        refId: download_count
        resultFormat: time_series
        select:
            - - params:
                    - count
                type: field
              - params: []
                type: mean
        tags:
            - key: host
              operator: =
              value: {{ worker }}
    - refId: condition
      relativeTimeRange:
        from: 600
        to: 0
      datasourceUid: __expr__
      model:
        conditions:
            - evaluator:
                params:
                    - 5.24288e+06
                type: lt
              operator:
                type: and
              query:
                params:
                    - download_rate
              reducer:
                type: max
            - evaluator:
                params:
                    - 0
                    - 0
                type: gt
              operator:
                type: and
              query:
                params:
                    - download_count
              reducer:
                params: []
                type: last
              type: query
        intervalMs: 1000
        maxDataPoints: 43200
        refId: condition
        type: classic_conditions
    execErrState: OK
    for: 4h
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: download_rate_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 65109
    title: '{{ worker }}: Download rate alert'
    uid: {{ (('download_rate_alert_' + worker ) | sha512)[:40] }}
- folder: openQA
  interval: 1m
  name: '{{ worker }}: Memory usage alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '572'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '12054'
      message: ''
    condition: A
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
            - -3e+09
            type: lt
          operator:
            type: and
          query:
            params:
            - B
          reducer:
            type: avg
        - evaluator:
            params:
            - 3e+09
            type: lt
          operator:
            type: or
          query:
            params:
            - B
          reducer:
            type: avg
        refId: A
        type: classic_conditions
      refId: A
    - datasourceUid: '000000001'
      model:
        alias: $col
        dsType: influxdb
        function: mean
        groupBy:
        - interval: auto
          params:
          - auto
          type: time
        - key: host
          params:
          - tag
          type: tag
        interval: 1m
        measurement: mem_inactive
        orderByTime: ASC
        policy: default
        query: SELECT mean(available) as available  FROM "mem" WHERE host = '{{ worker }}'
          AND $timeFilter GROUP BY time($interval), host ORDER BY asc
        rawQuery: true
        refId: B
        resultFormat: time_series
        select:
        - - params:
            - value
            type: field
          - params: []
            type: mean
        tags: []
      refId: B
      relativeTimeRange:
        from: 300
        to: 0
    execErrState: OK
    for: 1h
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: memory_usage_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 12054
    title: '{{ worker }}: Memory usage alert'
    uid: {{ (('memory_usage_alert_' + worker ) | sha512)[:40] }}
- folder: openQA
  interval: 1m
  name: '{{ worker }}: Memory usage absolute alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '572'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '12054'
      message: ''
    condition: A
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
             5.368709e+08
            type: lt
          operator:
            type: and
          query:
            params:
            - Available
          reducer:
            type: avg
        refId: A
        type: classic_conditions
      refId: A
    - datasourceUid: '000000001'
      model:
        alias: available_total
        dsType: influxdb
        function: mean
        groupBy:
        - interval: auto
          params:
          - auto
          type: time
        - key: host
          params:
          - tag
          type: tag
        interval: 1m
        measurement: mem_inactive
        orderByTime: ASC
        policy: default
        query: SELECT mean(available) + mean(swap_free) as available_total FROM "mem" WHERE
          host = '{{ worker }}' AND $timeFilter GROUP BY time($interval), host ORDER
          BY asc
        rawQuery: true
        refId: Available_total
        resultFormat: time_series
        select:
        - - params:
            - value
            type: field
          - params: []
            type: mean
        tags: []
      refId: Available_total
      relativeTimeRange:
        from: 300
        to: 0
    execErrState: OK
    for: 20m
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: memory_usage_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 12054
    title: '{{ worker }}: Memory usage absolute alert'
    uid: {{ (('memory_usage_absolute_alert_' + worker ) | sha512)[:40] }}
- folder: openQA
  interval: 1m
  name: '{{ worker }}: NTP offset alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '579'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '65102'
      message: ''
    condition: B
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: '000000001'
      model:
        alias: Stratum $tag_stratum
        groupBy:
        - params:
          - $__interval
          type: time
        - params:
          - stratum
          type: tag
        - params:
          - 'null'
          type: fill
        interval: 1m
        measurement: chrony
        orderByTime: ASC
        policy: default
        query: SELECT mean(abs("rms_offset")) FROM "chrony" WHERE ("host" = '{{ worker }}')
          AND $timeFilter GROUP BY time($__interval), "stratum" fill(null)
        rawQuery: false
        refId: A
        resultFormat: time_series
        select:
        - - params:
            - rms_offset
            type: field
          - params: []
            type: mean
        tags:
        - key: host
          operator: =
          value: {{ worker }}
      refId: A
      relativeTimeRange:
        from: 300
        to: 0
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
            - 1000
            - -1000
            type: outside_range
          operator:
            type: and
          query:
            params:
            - A
          reducer:
            type: avg
        refId: B
        type: classic_conditions
      refId: B
    execErrState: OK
    for: 20m
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: ntp_offset_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 65102
    title: '{{ worker }}: NTP offset alert'
    uid: {{ (('ntp_offset_alert_' + worker ) | sha512)[:40] }}
{% if host_interface %}
- folder: openQA
  interval: 1m
  name: '{{ worker }}: OpenQA Ping time alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '573'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '65098'
      message: ''
    condition: B
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: '000000001'
      model:
        groupBy:
        - params:
          - $__interval
          type: time
        - params:
          - 'null'
          type: fill
        interval: 1m
        measurement: ping
        orderByTime: ASC
        policy: default
        refId: A
        resultFormat: time_series
        select:
        - - params:
            - average_response_ms
            type: field
          - params: []
            type: mean
        tags:
        - key: url
          operator: =
          value: {{ host_interface }}
      refId: A
      relativeTimeRange:
        from: 300
        to: 0
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
            - 200
            type: gt
          operator:
            type: and
          query:
            params:
            - A
          reducer:
            type: avg
        refId: B
        type: classic_conditions
      refId: B
    execErrState: OK
    for: 5m
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: openqa_ping_time_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 65098
    title: '{{ worker }}: OpenQA Ping time alert'
    uid: {{ (('openqa_ping_time_alert_' + worker ) | sha512)[:40] }}
{% endif %}
- folder: openQA
  interval: 1m
  name: '{{ worker }}: QA network infrastructure Ping time alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '575'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '65099'
      message: ''
    condition: B
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: '000000001'
      model:
        alias: $tag_url
        groupBy:
        - params:
          - $__interval
          type: time
        - params:
          - url
          type: tag
        - params:
          - 'null'
          type: fill
        interval: 1m
        measurement: ping
        orderByTime: ASC
        policy: default
        refId: A
        resultFormat: time_series
        select:
        - - params:
            - average_response_ms
            type: field
          - params: []
            type: mean
        tags:
        - key: host
          operator: =
          value: {{ worker }}
      refId: A
      relativeTimeRange:
        from: 300
        to: 0
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
            - 200
            type: gt
          operator:
            type: and
          query:
            params:
            - A
          reducer:
            type: avg
        refId: B
        type: classic_conditions
      refId: B
    execErrState: OK
    for: 1h
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: ping_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 65099
    title: '{{ worker }}: QA network infrastructure Ping time alert'
    uid: {{ (('ping_alert_' + worker ) | sha512)[:40] }}
- folder: openQA
  interval: 1m
  name: '{{ worker }}: Too many Minion job failures alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '580'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '65104'
      message: |-
        Too many Minion jobs have failed on {{ worker }}.

        Review failed jobs on http://localhost:9530/minion/jobs?state=failed after tunneling the worker's Minion dashboard via `ssh -L 9530:localhost:9530 -N {{ worker }}`. Create a ticket if there's not already one. For the general log of the Minion job queue, checkout `journalctl -u openqa-worker-cacheservice.service -u openqa-worker-cacheservice-minion.service`. To remove all failed jobs on the machine:

        ```
        /usr/share/openqa/script/openqa-workercache eval 'my $jobs = app->minion->jobs({states => ["failed"]}); while (my $job = $jobs->next) { $job->remove }'
        ```
    condition: B
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: '000000001'
      model:
        alias: Active
        groupBy:
        - params:
          - $__interval
          type: time
        - params:
          - previous
          type: fill
        measurement: openqa_minion_jobs
        orderByTime: ASC
        policy: default
        refId: A
        resultFormat: time_series
        select:
        - - params:
            - active
            type: field
          - params: []
            type: mean
        tags:
        - key: host
          operator: =
          value: {{ worker }}
      refId: A
      relativeTimeRange:
        from: 300
        to: 0
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
            - 2000
            type: gt
          operator:
            type: and
          query:
            params:
            - A
          reducer:
            type: last
        - evaluator:
            params:
            - 100
            type: gt
          operator:
            type: or
          query:
            params:
            - C
          reducer:
            type: avg
        refId: B
        type: classic_conditions
      refId: B
    - datasourceUid: '000000001'
      model:
        alias: Failed
        groupBy:
        - params:
          - $__interval
          type: time
        - params:
          - previous
          type: fill
        measurement: openqa_minion_jobs
        orderByTime: ASC
        policy: default
        refId: C
        resultFormat: time_series
        select:
        - - params:
            - failed
            type: field
          - params: []
            type: mean
        tags:
        - key: host
          operator: =
          value: {{ worker }}
      refId: C
      relativeTimeRange:
        from: 300
        to: 0
    execErrState: OK
    for: 5m
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: minion_alerts_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 65104
    title: '{{ worker }}: Too many Minion job failures alert'
    uid: {{ (('minion_alerts_' + worker ) | sha512)[:40] }}
{% if host_interface %}
- folder: openQA
  interval: 5m
  name: '{{ worker }}: host up alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '574'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '65105'
      message: |-
        No data received for pings from worker to central host, likely host is down (or split network).

        See https://progress.opensuse.org/issues/71098 for details
    condition: B
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: '000000001'
      model:
        groupBy:
        - params:
          - $__interval
          type: time
        - params:
          - 'null'
          type: fill
        interval: 1m
        measurement: ping
        orderByTime: ASC
        policy: default
        refId: A
        resultFormat: time_series
        select:
        - - params:
            - result_code
            type: field
          - params: []
            type: mean
        tags:
        - key: url
          operator: =
          value: {{ host_interface }}
      refId: A
      relativeTimeRange:
        from: 300
        to: 0
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
            - 0
            type: gt
          operator:
            type: and
          query:
            params:
            - A
          reducer:
            type: avg
        refId: B
        type: classic_conditions
      refId: B
    execErrState: OK
    for: 1h30m
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: host_up_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
      alert: host_up
    noDataState: OK
    panelId: 65105
    title: '{{ worker }}: host up alert'
    uid: {{ (('host_up_alert_' + worker ) | sha512)[:40] }}
{% endif %}
- folder: openQA
  interval: 1m
  name: '{{ worker }}: partitions usage (%) alert'
  orgId: 1
  rules:
  - annotations:
      __alertId__: '576'
      __dashboardUid__: WD{{ worker }}
      __panelId__: '65090'
      message: ''
    condition: A
    dashboardUid: WD{{ worker }}
    data:
    - datasourceUid: __expr__
      model:
        conditions:
        - evaluator:
            params:
            - 200
            type: gt
          operator:
            type: and
          query:
            params:
            - B
          reducer:
            type: avg
        - evaluator:
            params:
            - {{ pillar.get('commonconf', {}).get(worker, {}).get('partitions_usage_alert_threshold', '85') }}
            type: gt
          operator:
            type: or
          query:
            params:
            - B
          reducer:
            type: avg
        refId: A
        type: classic_conditions
      refId: A
    - datasourceUid: '000000001'
      model:
        alias: $tag_device ($tag_fstype)
        dsType: influxdb
        function: mean
        groupBy:
        - interval: auto
          params:
          - auto
          type: time
        - key: host
          params:
          - tag
          type: tag
        - key: path
          params:
          - tag
          type: tag
        interval: 1m
        measurement: disk_total
        orderByTime: ASC
        policy: default
        query: SELECT mean("used_percent") AS "used_percent" FROM "disk" WHERE ("host"
          = '{{ worker }}' AND fstype !~ /^nfs|udf|iso9660/) AND $timeFilter GROUP BY time($interval),
          "device", "fstype" fill(null)
        rawQuery: true
        refId: B
        resultFormat: time_series
        select:
        - - params:
            - value
            type: field
          - params: []
            type: mean
        tags: []
      refId: B
      relativeTimeRange:
        from: 300
        to: 0
    execErrState: OK
    for: 5m
    isPaused: false
    labels:
      __contacts__: '"osd-admins"'
      rule_uid: partitions_usage_alert_{{ worker }}
      type: worker
      hostname: {{ worker }}
    noDataState: OK
    panelId: 65090
    title: '{{ worker }}: partitions usage (%) alert'
    uid: {{ (('partitions_usage_alert_' + worker ) | sha512)[:40] }}
- orgId: 1
  name: '{{ worker }}: System load alert'
  folder: openQA
  interval: 5m
  rules:
    - uid: {{ (('system_load_alert_' + worker ) | sha512)[:40] }}
      title: '{{ worker }}: System load alert'
      condition: C
      data:
        - refId: A
          relativeTimeRange:
            from: 10800
            to: 0
          datasourceUid: "000000001"
          model:
            adhocFilters: []
            alias: $col
            datasource:
                type: influxdb
                uid: "000000001"
            dsType: influxdb
            function: mean
            groupBy:
                - interval: auto
                  params:
                    - auto
                  type: time
                - key: host
                  params:
                    - tag
                  type: tag
            intervalMs: 900000
            limit: ""
            maxDataPoints: 43200
            measurement: system_load1
            orderByTime: ASC
            policy: default
            query: SELECT mean(load15) as load15 FROM "system" WHERE host = '{{ worker }}' AND $timeFilter GROUP BY time($interval), * ORDER BY asc
            rawQuery: true
            refId: A
            resultFormat: time_series
            select:
                - - params:
                        - value
                    type: field
                  - params: []
                    type: mean
            slimit: ""
            tags: []
            tz: ""
        - refId: B
          relativeTimeRange:
            from: 10800
            to: 0
          datasourceUid: __expr__
          model:
            conditions:
                - evaluator:
                    params: []
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                        - B
                  reducer:
                    params: []
                    type: last
                  type: query
            datasource:
                type: __expr__
                uid: __expr__
            expression: A
            intervalMs: 1000
            maxDataPoints: 43200
            reducer: last
            refId: B
            settings:
                mode: dropNN
            type: reduce
        - refId: C
          relativeTimeRange:
            from: 10800
            to: 0
          datasourceUid: __expr__
          model:
            conditions:
                - evaluator:
                    params:
                        - 60
                    type: gt
                  operator:
                    type: and
                  query:
                    params:
                        - C
                  reducer:
                    params: []
                    type: last
                  type: query
            datasource:
                type: __expr__
                uid: __expr__
            expression: B
            intervalMs: 1000
            maxDataPoints: 43200
            refId: C
            type: threshold
      dashboardUid: WD{{ worker }}
      panelId: 54694
      noDataState: OK
      execErrState: OK
      for: 90m
      annotations:
        __dashboardUid__: WD{{ worker }}
        __panelId__: "54694"
        description: |-
            System Load is considered too high for a longer time. Machine possibly overloaded. Especially when there are too many openQA worker instances configured openQA tests would become flaky and showing lost characters or repeated characters in VNC typing.

            Take a look which processes make the machine busy and look for corresponding openQA tests failing due to this situation and handle accordingly, e.g. retrigger the openQA tests after mitigating the root cause.

            See
            https://progress.opensuse.org/issues/150983
            for details.
        summary: System Load too high for a longer time, see https://progress.opensuse.org/issues/150983
      isPaused: false
      labels:
        __contacts__: '"osd-admins"'
        rule_uid: system_load_alert_{{ worker }}
        type: worker
        hostname: {{ worker }}

