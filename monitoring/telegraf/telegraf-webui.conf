[global_tags]
[agent]
  interval = "10s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "5s"
  precision = ""
  debug = false
  quiet = false
  logfile = ""
  hostname = ""
  omit_hostname = false

# use multiple [[inputs.ping]] sections so one unresolvable hostname does not prevent
# all other ping data to be gathered
{% set interfaces = salt['mine.get']('*', 'network.interfaces').keys()|list %}
{% for interface in interfaces -%}
[[inputs.ping]]
  urls = ['{{ interface }}']
  count = 1
  ping_interval = 1.0
  timeout = 1.0
  deadline = 10
{% endfor %}

[[inputs.http]]
  urls = [ "https://{{grains['fqdn']}}/admin/influxdb/jobs" ]
  data_format = "influx"
  timeout = "20s"
  interval = "30s"

[[inputs.http]]
  urls = [ "https://{{grains['fqdn']}}/admin/influxdb/minion" ]
  data_format = "influx"
  timeout = "20s"
  interval = "1m"

[[inputs.http_response]]
  urls = [ "https://{{grains['fqdn']}}/tests" ]
  response_timeout = "20s"
  interval = "1m"

[[inputs.apache]]
  urls = ["http://localhost/server-status?auto"]

[[inputs.postgresql]]
  address = "postgres://telegraf:telegraf@localhost"
  databases = ["openqa"]

[[inputs.postgresql_extensible]]
  address = "postgres://telegraf:telegraf@localhost"

  [[inputs.postgresql_extensible.query]]
    sqlquery="SELECT COUNT(usename) as open_connections, usename AS user FROM pg_stat_activity WHERE usename not like '' GROUP BY usename;"
    tagvalue="user"

[[inputs.postgresql_extensible]]
  address = "postgres://telegraf:telegraf@localhost/openqa"
  databases = ["openqa"]

{% set incomplete_ignore_reasons = "result='incomplete' and (reason is null or (reason not like 'quit%' and reason not like 'tests died%' and reason not like 'isotovideo died: Could not find % in complete history%'))" %}
  [[inputs.postgresql_extensible.query]]
    sqlquery="select count(id) as \"incompletes_last_24h\" from jobs where {{ incomplete_ignore_reasons }} and t_finished >= timezone('UTC', now()) - interval '24 hour'"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select count(id) as \"incompletes_not_restarted_last_24h\" from jobs where {{ incomplete_ignore_reasons }} and clone_id is null and t_finished >= timezone('UTC', now()) - interval '24 hour'"

[[inputs.postgresql_extensible]]
  address = "postgres://telegraf:telegraf@localhost/openqa"
  databases = ["openqa"]
  interval = "2m"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select group_id, (select concat_ws('/', (select name from job_group_parents where id = parent_id), name) from job_groups where id = group_id) as group_name, sum(result_size)::bigint as result_size from jobs group by group_id order by group_id;"
    withdbname=false
    tagvalue="group_id,group_name"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select machine as machine_class,percentile_disc(.5) within group (order by age) as job_age_p50, percentile_disc(.9) within group (order by age) as job_age_p90, percentile_disc(.99) within group (order by age) as job_age_p99, percentile_disc(1) within group (order by age) as job_age_p100 from (select id,state,machine,arch,test,cast(extract(epoch from ((timezone('UTC', now()) - (t_created at time zone 'UTC')))) as integer) as age from jobs where state = 'scheduled' or state = 'assigned' order by age) as job_ages group by machine,arch"
    tagvalue="machine_class"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select count(id) as broken_workers from workers where error is not null and t_updated > (timezone('UTC', now()) - interval '1 hour') and not error like 'graceful disconnect%' and not error like '%Cache service queue already full %'"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select (select count(*) filter (where result='failed') * 100. / count(*) from jobs)::numeric(5,2)::float as ratio_failed_p"

  [[inputs.postgresql_extensible.query]]
    sqlquery="with mm_jobs as (select distinct id, result from jobs left join job_dependencies on (id = child_job_id or id = parent_job_id) where t_created >= (select timezone('UTC', now()) - interval '24 hour') and dependency = 2) select result, round(count(id) * 100. / (select count(id) from mm_jobs), 2)::numeric(5,2)::float as ratio_mm from mm_jobs group by mm_jobs.result"
    tagvalue="result"

[[inputs.postgresql_extensible]]
  address = "postgres://telegraf:telegraf@localhost/openqa"
  databases = ["openqa"]
  interval = "24h"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select result, round(count(id) * 100. / (select count(id) from jobs), 2)::numeric(5,2)::float as ratio_all_long_term from jobs group by result"
    tagvalue="result"

  [[inputs.postgresql_extensible.query]]
    sqlquery="with mm_jobs as (select distinct id, result from jobs left join job_dependencies on (id = child_job_id or id = parent_job_id) where dependency = 2) select result, round(count(id) * 100. / (select count(id) from mm_jobs), 2)::numeric(5,2)::float as ratio_mm_long_term from mm_jobs group by mm_jobs.result"
    tagvalue="result"

  [[inputs.postgresql_extensible.query]]
    sqlquery="with finished as (select result, host from jobs left join workers on jobs.assigned_worker_id = workers.id where result != 'none') select host, round(count(*) filter (where result='failed') * 100. / count(*), 2)::numeric(5,2)::float as ratio_failed_by_host from finished where host is not null group by host"
    tagvalue="host"

[[inputs.tail]]
  files = ["/var/log/apache2/openqa.access_log"]
  interval = "30s"
  from_beginning = false
  name_override = "apache_log"
  ## For parsing logstash-style "grok" patterns:
  data_format = "grok"
  grok_patterns = ["%{CUSTOM_LOG}"]
  grok_custom_pattern_files = []
  grok_custom_patterns = '''
      CUSTOM_LOG %{COMBINED_LOG_FORMAT} %{NUMBER:response_time_us:int}
  '''

{# sync with monitoring/grafana/webui.services.json - and do not reorder, grafana uses the loop index as id #}
{% for service in ['sshd','openqa-gru','openqa-webui','openqa-livehandler','openqa-scheduler','openqa-websockets','smb','vsftpd','telegraf','salt-master','salt-minion','rsyncd','postgresql','postfix','cron','apache2'] %}
[[inputs.procstat]]
  cgroup = "systemd/system.slice/{{ service }}.service"
  fieldpass = ["pid_count"]
  interval = "30s"
{% endfor %}

[[inputs.netstat]]

[[inputs.procstat]]
  pattern = ".*"
  fieldpass = ["memory_usage", "cpu_usage"]
