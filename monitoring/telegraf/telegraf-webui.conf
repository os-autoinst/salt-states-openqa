[global_tags]
[agent]
  interval = "10s"
  round_interval = true
  metric_batch_size = 1000
  metric_buffer_limit = 10000
  collection_jitter = "0s"
  flush_interval = "10s"
  flush_jitter = "0s"
  precision = ""
  debug = false
  quiet = false
  logfile = ""
  hostname = ""
  omit_hostname = false

# use multiple [[inputs.ping]] sections so one unresolvable hostname does not prevent
# all other ping data to be gathered
{% set interfaces = salt['mine.get']('*', 'network.interfaces').keys()|list %}
{% for interface in interfaces -%}
[[inputs.ping]]
  urls = ['{{ interface }}']
  count = 1
  ping_interval = 1.0
  timeout = 1.0
  deadline = 10
{% endfor %}

[[inputs.http]]
  urls = [ "https://{{grains['fqdn']}}/admin/influxdb/jobs" ]
  data_format = "influx"
  timeout = "20s"
  interval = "30s"

[[inputs.http]]
  urls = [ "https://{{grains['fqdn']}}/admin/influxdb/minion" ]
  data_format = "influx"
  timeout = "20s"
  interval = "1m"

[[inputs.http_response]]
  urls = [ "https://{{grains['fqdn']}}/tests" ]
  interval = "1m"

[[inputs.apache]]
  urls = ["http://localhost/server-status?auto"]

[[inputs.postgresql]]
  address = "postgres://telegraf:telegraf@localhost"
  databases = ["openqa"]

[[inputs.postgresql_extensible]]
  address = "postgres://telegraf:telegraf@localhost"

  [[inputs.postgresql_extensible.query]]
    sqlquery="SELECT COUNT(usename) as open_connections, usename AS user FROM pg_stat_activity WHERE usename not like '' GROUP BY usename;"
    tagvalue="user"

[[inputs.postgresql_extensible]]
  address = "postgres://telegraf:telegraf@localhost/openqa"
  databases = ["openqa"]

  [[inputs.postgresql_extensible.query]]
    sqlquery="select count(id) as \"incompletes_last_24h\" from jobs where result='incomplete' and (reason is null or (reason not like 'quit%' and reason not like 'isotovideo died: Could not find % in complete history%')) and t_finished >= NOW() - interval '24 hour'"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select count(id) as \"incompletes_not_restarted_last_24h\" from jobs where result='incomplete' and (reason is null or (reason not like 'quit%' and reason not like 'isotovideo died: Could not find % in complete history%')) and clone_id is null and t_finished >= NOW() - interval '24 hour'"

[[inputs.postgresql_extensible]]
  address = "postgres://telegraf:telegraf@localhost/openqa"
  databases = ["openqa"]
  interval = "2m"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select group_id, (select concat_ws('/', (select name from job_group_parents where id = parent_id), name) from job_groups where id = group_id) as group_name, sum(result_size)::bigint as result_size from jobs group by group_id order by group_id;"
    withdbname=false
    tagvalue="group_id,group_name"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select machine as machine_class,percentile_disc(.5) within group (order by age) as job_age_p50, percentile_disc(.9) within group (order by age) as job_age_p90, percentile_disc(.99) within group (order by age) as job_age_p99, percentile_disc(1) within group (order by age) as job_age_p100 from (select id,state,machine,arch,test,cast(extract(epoch from ((NOW() - (t_created at time zone 'UTC')))) as integer) as age from jobs where state = 'scheduled' or state = 'assigned' order by age) as job_ages group by machine,arch"
    tagvalue="machine_class"

  [[inputs.postgresql_extensible.query]]
    sqlquery="select count(id) as broken_workers from workers where error is not null and t_updated > (timezone('UTC', now()) - interval '1 hour') and not error like 'graceful disconnect%'"

[[inputs.logparser]]
  files = ["/var/log/apache2/openqa.access_log"]
  from_beginning = false
  name_override = "apache_log"
  ## For parsing logstash-style "grok" patterns:
  [inputs.logparser.grok]
    patterns = ["%{CUSTOM_LOG}"]
    custom_patterns = '''
      CUSTOM_LOG %{COMBINED_LOG_FORMAT} %{NUMBER:response_time_us:int}
    '''

[[inputs.exec]]
  commands = [ '/etc/telegraf/scripts/logwarn_openqa_formater.sh' ]
  data_format = "influx"
  interval = "2m"

{# sync with monitoring/grafana/webui.services.json - and do not reorder, grafana uses the loop index as id #}
{% for service in ['sshd','openqa-gru','openqa-webui','openqa-livehandler','openqa-scheduler','openqa-websockets','smb','vsftpd','telegraf','salt-master','salt-minion','rsyncd','postgresql','postfix','cron','apache2'] %}
[[inputs.procstat]]
  cgroup = "systemd/system.slice/{{ service }}.service"
{% endfor %}
